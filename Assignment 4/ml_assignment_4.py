# -*- coding: utf-8 -*-
"""ML_Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L9Nh5Pr17MeO-aWiltY2QHMEYqV1ng-e

"I certify that the code and data in this assignment were generated independently, using only the tools
and resources defined in the course and that I did not receive any external help, coaching or contributions
during the production of this work."
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces
import time
from google.colab import widgets
import random
import numpy as np

grid = np.zeros((4, 4))

print(grid)

#Starting position
pos1 = [0, 0]

#Terminal position
pos2 = [3, 3]

grid[tuple(pos1)] = 1
grid[tuple(pos2)] = 1.5

print(grid)
plt.imshow(grid)
plt.show()

output_grid = widgets.Grid(1,1)
for _ in range(10):
  grid = np.zeros((4, 4))
  grid[np.random.randint(3), np.random.randint(3)] = 1
  grid[3,3]=0.7
  grid[2,2]=0.6
  grid[1,3]=.05
  with output_grid.output_to(0, 0):
    output_grid.clear_cell()
    plt.imshow(grid)

  time.sleep(5)

class GridEnvironment(gym.Env):

  def __init__(self):
    self.observation_space = spaces.Discrete(16)
    self.action_space = spaces.Discrete(4)
    self.max_timesteps = 10

  def reset(self):
    self.timestep = 0
    self.agent_pos = [0, 0]
    self.goal_pos = [3, 3]
    self.inter_pos=[2,2]
    self.inter_pos2=[0,3]
    self.state = np.zeros((4, 4))
    self.state[tuple(self.agent_pos)] = 1
    self.state[tuple(self.goal_pos)] = 0.75
    self.state[tuple(self.inter_pos)]=0.4
    self.state[tuple(self.inter_pos2)]=0.7

    observation = self.state.flatten()

    return observation

  def step(self, action):
    randomN = np.random.random(1)[0]

    if action == 0:
        self.agent_pos[0] += -1
    elif action == 1:
        self.agent_pos[0] -= -1
    elif action == 2:
        self.agent_pos[1] += 1
    elif action == 3:
        self.agent_pos[1] -= 1
    
    #Define the clipping boundary

    self.agent_pos = np.clip(self.agent_pos, 0, 3)
    self.agent_pos = np.clip(self.agent_pos, 0, 3)
    self.state = np.zeros((4,4))
    self.state[tuple(self.agent_pos)] = 1
    self.state[tuple(self.goal_pos)] = 0.5
    observation = self.state.flatten()

    reward = 0
    if (self.agent_pos == self.goal_pos).all():
      reward = 1
    
    elif (self.agent_pos == self.inter_pos).all():
      reward =+1.85

    elif (self.agent_pos == self.inter_pos2).all():
      reward=-0.5
      self.timestep += 1
    done = True if self.timestep >= self.max_timesteps else False
    status = {}

    return observation, reward, done, status

  def render(self):
    plt.imshow(self.state)

env = GridEnvironment()
obs = env.reset()
env.render()

#Random Action
action = 2
reward = 0.7
for action in [1,0,1,2,3,3,3,3,1,1,1,2,2,2,0,0,0,0]:
  obs, reward, done, _ = env.step(action)
  with output_grid.output_to(0, 0):
    output_grid.clear_cell()
    env.render()
  time.sleep(1)
#print('Reward: ', reward)

action = 2
reward = 2
for action in [1,0,1,2,3,3,3,3,1,1,1,2,2,2,0,0,0,0]:
  obs, reward, done, _ = env.step(action)
  with output_grid.output_to(0, 0):
    output_grid.clear_cell()
    env.render()
  time.sleep(1)
#print('Reward: ', reward)

action = 2
reward2 = 0
output_grid = widgets.Grid(4,4)
row=0
col=0
for action in [1,0,1,2,3,3,3,1,1,1,2,2,2,0,0,0,0,0]:
  obs, reward, done, _ = env.step(action)
  reward2+=reward
  print(reward)
  with output_grid.output_to(row, col):
    output_grid.clear_cell()
    
    env.render()
  col+=1
  if col>3:
      col = 0
      row+=1
  if row>3:
      break
  time.sleep(1)
print('Total Reward accumulated ', reward2)

"""# Part 2

We once again define the class grid Enviroment
"""

class GridEnvironment(gym.Env):
    def __init__(self, gamma, epsilon, epsilonDecay, alpha, episodes = 20, maxTimesteps = 10, rows = 4, cols = 4):
        self.eventRecord = []
        self.rows = rows
        self.cols = cols
        self.grid=np.zeros((3,3))
        self.observation_space = spaces.Discrete(self.rows*self.cols)
        self.actions = {"UP":0, "RIGHT":1, "DOWN":2,"LEFT":3}
        self.maxTimesteps = maxTimesteps
        self.state = np.zeros((self.rows, self.cols))
        self.timestep = 0
        self.agent_pos = [0, 0]
        self.goal_pos = [self.rows-1, self.cols-1]
        self.negative_r = [1,1]
        self.bonus_pos = [0,3]
        self.state = np.zeros((self.rows, self.cols))
        self.env_state=np.zeros((3,3))
        self.reward = {tuple(self.goal_pos): 2, tuple(self.negative_r) : -1, tuple(self.bonus_pos): 1}
        self.rewardStates = {tuple(self.goal_pos), tuple(self.negative_r), tuple(self.bonus_pos)}
        self.state[tuple(self.agent_pos)] = 2
        self.state[tuple(self.goal_pos)] = 0.5
        self.state[tuple(self.negative_r)] = 1.5
        self.state[tuple(self.red_flag)] = -0.5
        self.state[tuple(self.bonus_pos)] = 2.5
        self.stat=(self.grid==self.env_state)
        self.done=self.stat
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilonDecay = epsilonDecay
        self.alpha = alpha
        self.episodes = episodes
        self.maxTimesteps = maxTimesteps
        self.cumulativeReward = 0
        self.done = False

        zero_state=[0,0,0,0]
        self.qTable = {}
        for i in range(self.rows):
            for j in range(self.cols):
                self.qTable[tuple([i,j])] = zero_state
    
    def reset(self):
        #print("Episode ends")
        self.count+=1
        self.timestep = 0
        self.agent_pos = [0, 0]
        self.goal_pos = [3, 3]
        self.red_flag = [1,1]
        self.inter_reward = [0,3]
        self.inter_reward2 = [2,2]
        self.state = np.zeros((4, 4))

        self.state[tuple(self.agent_pos)] = 2
        self.state[tuple(self.goal_pos)] = 0.5
        self.state[tuple(self.inter_reward)] = 1.5
        self.state[tuple(self.inter_reward2)]=2.5
        self.state[tuple(self.red_flag)] = -0.5

        self.cumulativeReward = 0
        self.event = []

        observation = self.state.flatten()
        return observation

    
    def render(self):
        plt.imshow(self.state)

    def step(self, action):
        randomN = np.random.random(1)[0]

        if action == 0:
            self.agent_pos[0] += -1
        elif action == 1:
            self.agent_pos[0] -= -1
        elif action == 2:
            self.agent_pos[1] += 1
        elif action == 3:
            self.agent_pos[1] -= 1
    
    #Define the clipping boundary
        self.agent_pos = np.clip(self.agent_pos, 0, 3)
        self.agent_pos = np.clip(self.agent_pos, 0, 3)
        self.state = np.zeros((4,4))
        self.state[tuple(self.agent_pos)] = 1
        self.state[tuple(self.goal_pos)] = 0.5
        observation = self.state.flatten()

        reward = 0
        if (self.agent_pos == self.goal_pos).all():
            reward = 1
        elif (self.agent_pos == self.inter_pos).all():
            reward =+2
        elif (self.agent_pos == self.inter_pos2).all():
            reward=-0.5
        self.timestep += 1
        done = True if self.timestep >= self.max_timesteps else False
        status = {}
        return observation, reward, done, status
    
    def getValidActions(self):
        actionAvailable = self.possibleAction(self.agent_pos)
        stateAvailable = self.getAllFinalPosition(actionAvailable)
        finalaction, finalstate = self.getStateActionPair(actionAvailable, stateAvailable)
        return finalaction

    def possible_actions(self, pos):
        possible_actions = [0,1,2,3]
        
        changed_sa=[]
        most_common_act=None
        new_possible_actions=possible_actions.copy()

        if pos[1]==0: 
          changed_sa.append(1)
          new_possible_actions.remove(3)
        if pos[1]==self.cols-1:
            changed_sa.append(1)
            new_possible_actions.remove(1)
        if pos[0]==0:
            new_possible_actions.remove(0)
            changed_sa.append(0)
        if pos[0]==self.rows-1:
            changed_sa.append(0)
            new_possible_actions.remove(2)
        if pos[0]==self.rows-1 and pos[1]==self.cols-1:
            self.done = True
        
        import collections
        counter=collections.Counter(changed_sa)
        
        most_common_act=counter.values()
        freq= counter()
        return new_possible_actions

    def getFinalPosAfterAction(self, action):
        nextState = self.agent_pos.copy()
        if action == 0:
            nextState[0] -=1
        elif action == 1:
            nextState[1] +=1
        elif action == 2:
            nextState[0] +=1
        elif action == 3:
            nextState[1] -=1

        if nextState[0]<0:
            nextState[0] = 0
        elif nextState[1]<0:
            nextState[1] = 0
        elif nextState[0]>=self.rows:
            nextState[0] = self.rows-1
        elif nextState[1]>=self.cols:
            nextState[1] = self.cols-1

        return nextState

    def getReward(self,pos):
        if tuple(pos) not in self.rewardStates:
            return -0.2
        else:
            return self.reward[tuple(pos)]

    def getStateActionPair(self, possibleAction, possibleStates):
        action = -1
        state = [-1,-1]
        #Random Epsilon
        if np.random.uniform(0,1) <= self.epsilon:
            action, state, _, _ = self.greedy_Action(possibleAction, possibleStates)
            remainingActions, remainingStates = [], []
            for i in possibleAction:
                if i!=action:
                    remainingActions.append(i)
            for i in possibleStates:
                if not (i[0]==state[0] and i[1]==state[1]):
                    remainingStates.append(i)
            action = random.choice(remainingActions)
            state = remainingStates[remainingActions.index(action)]
        else:
            action, state, _, _ = self.greedy_Action(possibleAction, possibleStates)
        return action, state

    def greedy_Action(self,possibleActions, possibleStates):
        currentMax = None
        fstate = None
        faction = None
        qValues = None
        for action,state in zip(possibleActions, possibleStates):
            if state[0] == self.rows-1 or state[1] == self.cols-1:
                return action, state,0, 0 
            if currentMax == None:
                currentMax= self.qTable[tuple(self.agent_pos)][action]
                fstate = state
                faction = action
                qValues = self.qTable[tuple(self.agent_pos)]
            elif currentMax != None:
                max1 = self.qTable[tuple(self.agent_pos)][action]
                if max1 > currentMax:
                    currentMax = max1
                    fstate = state
                    faction = action
                    qValues = self.qTable[tuple(self.agent_pos)]
        return faction, fstate, currentMax, qValues


    def finQValuesSarsa(self):
        self.eventRecord.reverse()
        for i in range(len(self.eventRecord)-1):
            # QValue = Old_QValue + alpha * (CurrentReward + gamma * max(previousStateQValues) - Old_QValue)
            oldQvalue = self.qTable[tuple(self.eventRecord[i+1][0])][self.eventRecord[i][1]]
            allStates = [i[0] for i in self.eventRecord]
            actionChain = self.possibleAction(allStates)
            if not self.done:
                statesChain = self.getAllFinalPosition(actionChain, self.eventRecord[i][0])
                actionSelected, stateSelected = self.getStateActionPair(actionChain, statesChain)
                self.qTable[tuple(self.eventRecord[i+1][0])][self.eventRecord[i][1]] = oldQvalue + self.alpha * (self.eventRecord[i][2] + self.gamma * max(self.qTable[tuple(stateSelected)][actionSelected]) - oldQvalue)
            
            self.qTable[tuple(self.eventRecord[i+1][0])][self.eventRecord[i][1]] = oldQvalue + self.alpha * (self.eventRecord[i][2] - oldQvalue)

    def trainSarsa(self):
        self.rewardPerEpisode = []
        self.epsilonValues = [self.epsilon]
        episode = 0
        episodes = []
        for i in range(self.episodes):
            self.timestamp = 0
            self.done = False
            self.reset()
            while not self.done:
                observation, reward, self.done, _ = self.step(self.getValidActions())
                if self.done:
                    self.rewardPerEpisode.append(reward)
            self.finQValuesSarsa()
            if self.epsilon>0:
                self.epsilon -=self.epsilon*self.epsilonDecay
            self.epsilonValues.append(self.epsilon)
    
    def test(self):
        self.rewardPerEpisode = []
        self.epsilonValues = [self.epsilon]
        episode = 0
        episodes = []
        for i in range(100):
            self.timestamp = 0
            self.done = False
            self.reset()
            while not self.done:
                actionAvailable = self.possibleAction(self.agent_pos)
                stateAvailable = self.getAllFinalPosition(actionAvailable)
                faction, fstate, currentMax, qValues =  self.greedy_Action(actionAvailable, stateAvailable)
                observation, reward, self.done, _ = self.step(faction)
                if self.done:
                    self.rewardPerEpisode.append(reward)
            self.finQValuesSarsa()
            if self.epsilon>0:
                self.epsilon -=self.epsilon*self.epsilonDecay
            self.epsilonValues.append(self.epsilon)

"""# Vizualising results """

number_of_episodes=[10,100,500,1000]
ep=0.9
epsilonDecay=0.01
epsilon_val=[]
for i in range(1000):
    epsilon_val.append(ep)
    ep=(ep*(1-epsilonDecay))
plt.plot(epsilon_val)
plt.title("Epsilon Decay for 100 episodes with inital value 0.9 and decay rate 0.01")
plt.show()

for episodes in number_of_episodes:
    rewardDetSarsa = []

    qLearningEnv = GridEnvironment(gamma = 0.99, epsilon = 0.9 , epsilonDecay=0.01 , alpha=0.5, episodes = episodes, maxTimesteps=15)
    qLearningEnv.trainSarsa()
    rewardDetSarsa = qLearningEnv.rewardPerEpisode

    plt.plot(rewardDetSarsa,label="Sarsa", color = "red")
    plt.legend(loc='best')
    plt.title("Using Sarsa with  number of Episodes as: {}".format(episodes))
    plt.ylabel("Reward")
    plt.xlabel("Episodes")
    plt.show()

print(epsilon_val)

"""Hyperparameter Tuning and Bonus Not Attempted

References


1.https://ikvibhav.medium.com/open-aigym-simple-sarsa-and-q-learning-reinforcement-learning-implementations-7d5ea6f1ff9
2.https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html
3.https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action
4.Reinforcement Learning Project and assignment https://github.com/namantejaswi/Reinforcement_Learning_HW
"""